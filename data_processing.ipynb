{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "410eda29",
   "metadata": {},
   "source": [
    "# Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "abd500cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-11-07 14:07:24,558 [INFO] Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from ultralytics import YOLO\n",
    "from facial_emotion_recognition import EmotionRecognition\n",
    "import mediapipe as mp\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s [%(levelname)s] %(message)s\",\n",
    "    handlers=[logging.StreamHandler(sys.stdout)]\n",
    ")\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "logging.info(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c509968",
   "metadata": {},
   "source": [
    "## Silesian Deception Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80df03e6",
   "metadata": {},
   "source": [
    "### Face detection and crop (YOLO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7bf668a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_faces(model, frame):\n",
    "    results = model(frame, verbose=False)\n",
    "    if not results or results[0].boxes is None:\n",
    "        return []\n",
    "    return results[0].boxes.xyxy.int().tolist()\n",
    "\n",
    "def face_crop(model, frame):\n",
    "    boxes = detect_faces(model, frame)\n",
    "\n",
    "    for _, box in enumerate(boxes):\n",
    "        x1, y1, x2, y2 = map(int, box)\n",
    "        face_crop = frame[y1:y2, x1:x2]\n",
    "        if face_crop.size == 0:\n",
    "            continue\n",
    "        return face_crop\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b2f8282",
   "metadata": {},
   "source": [
    "### Resize images to consistent size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9d4b9ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_frame(frame, size=(224, 224)):\n",
    "    return cv2.resize(frame, size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53e41124",
   "metadata": {},
   "source": [
    "### Geometric face normalization with MediaPipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6d3ff58a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def geometric_normalization(frame, face_mesh):\n",
    "    LEFT_EYE_LANDMARKS = [33, 133]\n",
    "    RIGHT_EYE_LANDMARKS = [362, 263]\n",
    "\n",
    "    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    results = face_mesh.process(rgb_frame)\n",
    "\n",
    "    if not results.multi_face_landmarks:\n",
    "        return frame\n",
    "\n",
    "    landmarks = results.multi_face_landmarks[0].landmark\n",
    "    h, w, _ = frame.shape\n",
    "\n",
    "    left_eye = np.array([[landmarks[i].x * w, landmarks[i].y * h] for i in LEFT_EYE_LANDMARKS]).mean(axis=0)\n",
    "    right_eye = np.array([[landmarks[i].x * w, landmarks[i].y * h] for i in RIGHT_EYE_LANDMARKS]).mean(axis=0)\n",
    "\n",
    "    dy = right_eye[1] - left_eye[1]\n",
    "    dx = right_eye[0] - left_eye[0]\n",
    "    angle = np.degrees(np.arctan2(dy, dx))\n",
    "\n",
    "    center = tuple(map(float, np.mean([left_eye, right_eye], axis=0)))\n",
    "    rot_mat = cv2.getRotationMatrix2D(center, angle, 1.0)\n",
    "    aligned = cv2.warpAffine(frame, rot_mat, (w, h), flags=cv2.INTER_CUBIC)\n",
    "\n",
    "    return aligned"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24a155ee",
   "metadata": {},
   "source": [
    "### Emotion Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5a9dd561",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_emotion_probs(frame, emotion_detector):\n",
    "    if frame.ndim == 3:\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    tensor = emotion_detector.transform(frame).unsqueeze(0).to(emotion_detector.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = emotion_detector.network(tensor)\n",
    "        probs = torch.softmax(output, dim=1).cpu().numpy()[0]\n",
    "\n",
    "    return {emotion_detector.emotions[i]: float(probs[i]) for i in range(len(probs))}\n",
    "\n",
    "def detect_emotions(frame, emotion_detector):\n",
    "    return get_emotion_probs(frame, emotion_detector)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7078f123",
   "metadata": {},
   "source": [
    "### All together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7fb48240",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_video(video_path, face_detector, emotion_detector, face_mesh, frame_skip, label):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    video_name = os.path.splitext(os.path.basename(video_path))[0]\n",
    "\n",
    "    results = []\n",
    "\n",
    "    frame_idx = 0\n",
    "\n",
    "    logging.info(f'Processing video: {video_name}')\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        if frame_idx % frame_skip != 0:\n",
    "            frame_idx += 1\n",
    "            continue\n",
    "\n",
    "        face = face_crop(face_detector, frame)\n",
    "        if face is None:\n",
    "            frame_idx += 1\n",
    "            continue\n",
    "\n",
    "        resized_face = resize_frame(face)\n",
    "        normalized_face = geometric_normalization(resized_face, face_mesh)\n",
    "        emotions = detect_emotions(normalized_face, emotion_detector)\n",
    "\n",
    "        results.append({\n",
    "            'video': f\"{video_name}_{label}\",\n",
    "            'frame_idx': frame_idx,\n",
    "            'deceptive': label,\n",
    "            **emotions\n",
    "        })\n",
    "\n",
    "        frame_idx += 1\n",
    "\n",
    "    cap.release()\n",
    "    logging.info(f\"âœ… Finished video: {video_name} ({len(results)} frames processed)\")\n",
    "    return results\n",
    "\n",
    "\n",
    "def process_dataset(root_dir='data/silesian_deception_dataset', out_path='processed_data/silesian_deception_dataset/emotions.csv', frame_skip=5, device=device, deception_folder='poli2video'):\n",
    "    logging.info(\"ðŸš€ Starting dataset processing...\")\n",
    "    face_detector = YOLO('model_weights/yolov8n-face.pt').to(device)\n",
    "    emotion_detector = EmotionRecognition(device='gpu' if device == 'cuda' else 'cpu')\n",
    "\n",
    "    dataset = []\n",
    "\n",
    "    mp_face_mesh = mp.solutions.face_mesh\n",
    "    with mp_face_mesh.FaceMesh(\n",
    "        static_image_mode=True,\n",
    "        refine_landmarks=True,\n",
    "        max_num_faces=1\n",
    "    ) as face_mesh:\n",
    "        for folder in tqdm(os.listdir(root_dir), desc=\"Processing folders\", file=sys.stdout):\n",
    "            folder_path = os.path.join(root_dir, folder)\n",
    "            if not os.path.isdir(folder_path):\n",
    "                continue\n",
    "\n",
    "            for file in tqdm(os.listdir(folder_path), desc=f\"Processing videos in {folder}\", leave=False, file=sys.stdout, dynamic_ncols=True):\n",
    "                if not file.lower().endswith(\".avi\"):\n",
    "                    continue\n",
    "\n",
    "                video_path = os.path.join(folder_path, file)\n",
    "                label = folder.lower() == deception_folder\n",
    "                video_results = process_video(video_path, face_detector, emotion_detector, face_mesh, frame_skip, label)\n",
    "                dataset.extend(video_results)\n",
    "        \n",
    "    df = pd.DataFrame(dataset)\n",
    "    os.makedirs(os.path.dirname(out_path), exist_ok=True)\n",
    "    df.to_csv(out_path, index=False)\n",
    "\n",
    "    logging.info(\"ðŸŽ‰ All videos processed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86ed90b3",
   "metadata": {},
   "source": [
    "### Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "693b7b2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-11-07 14:07:24,625 [INFO] ðŸš€ Starting dataset processing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1762520844.933052   26720 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1762520844.980365   26770 gl_context.cc:357] GL version: 3.2 (OpenGL ES 3.2 NVIDIA 580.95.05), renderer: NVIDIA GeForce RTX 2060/PCIe/SSE2\n",
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*] Accuracy: 0.9565809379727686\n",
      "Processing folders:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0000 00:00:1762520844.982400   26769 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-11-07 14:07:24,987 [INFO] Processing video: person1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0000 00:00:1762520844.991974   26768 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "/home/pekoraptor/dev/lie-detection/.venv/lib/python3.10/site-packages/google/protobuf/symbol_database.py:55: UserWarning: SymbolDatabase.GetPrototype() is deprecated. Please use message_factory.GetMessageClass() instead. SymbolDatabase.GetPrototype() will be removed soon.\n",
      "  warnings.warn('SymbolDatabase.GetPrototype() is deprecated. Please '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-11-07 14:08:11,721 [INFO] âœ… Finished video: person1 (2377 frames processed)\n",
      "2025-11-07 14:08:11,724 [INFO] Processing video: person10\n",
      "2025-11-07 14:08:50,444 [INFO] âœ… Finished video: person10 (1992 frames processed)\n",
      "2025-11-07 14:08:50,455 [INFO] Processing video: person11\n",
      "2025-11-07 14:09:39,331 [INFO] âœ… Finished video: person11 (2520 frames processed)\n",
      "2025-11-07 14:09:39,342 [INFO] Processing video: person12\n",
      "2025-11-07 14:10:40,742 [INFO] âœ… Finished video: person12 (3153 frames processed)\n",
      "2025-11-07 14:10:40,753 [INFO] Processing video: person13\n",
      "2025-11-07 14:11:25,800 [INFO] âœ… Finished video: person13 (2310 frames processed)\n",
      "2025-11-07 14:11:25,810 [INFO] Processing video: person14\n",
      "2025-11-07 14:12:07,287 [INFO] âœ… Finished video: person14 (2132 frames processed)\n",
      "2025-11-07 14:12:07,291 [INFO] Processing video: person15\n",
      "2025-11-07 14:12:59,810 [INFO] âœ… Finished video: person15 (2688 frames processed)\n",
      "2025-11-07 14:12:59,820 [INFO] Processing video: person16\n",
      "2025-11-07 14:13:45,766 [INFO] âœ… Finished video: person16 (2347 frames processed)\n",
      "2025-11-07 14:13:45,777 [INFO] Processing video: person17\n",
      "2025-11-07 14:14:34,844 [INFO] âœ… Finished video: person17 (2499 frames processed)\n",
      "2025-11-07 14:14:34,855 [INFO] Processing video: person18\n",
      "2025-11-07 14:15:15,374 [INFO] âœ… Finished video: person18 (2058 frames processed)\n",
      "2025-11-07 14:15:15,384 [INFO] Processing video: person19\n",
      "2025-11-07 14:15:57,665 [INFO] âœ… Finished video: person19 (2157 frames processed)\n",
      "2025-11-07 14:15:57,675 [INFO] Processing video: person2\n",
      "2025-11-07 14:16:33,977 [INFO] âœ… Finished video: person2 (1862 frames processed)\n",
      "2025-11-07 14:16:33,987 [INFO] Processing video: person20\n",
      "2025-11-07 14:17:13,631 [INFO] âœ… Finished video: person20 (2023 frames processed)\n",
      "2025-11-07 14:17:13,640 [INFO] Processing video: person21\n",
      "2025-11-07 14:17:56,442 [INFO] âœ… Finished video: person21 (2196 frames processed)\n",
      "2025-11-07 14:17:56,446 [INFO] Processing video: person22\n",
      "2025-11-07 14:18:49,189 [INFO] âœ… Finished video: person22 (2702 frames processed)\n",
      "2025-11-07 14:18:49,192 [INFO] Processing video: person23\n",
      "2025-11-07 14:19:34,346 [INFO] âœ… Finished video: person23 (2318 frames processed)\n",
      "2025-11-07 14:19:34,357 [INFO] Processing video: person24\n",
      "2025-11-07 14:20:18,767 [INFO] âœ… Finished video: person24 (2288 frames processed)\n",
      "2025-11-07 14:20:18,777 [INFO] Processing video: person25\n",
      "2025-11-07 14:21:01,698 [INFO] âœ… Finished video: person25 (2193 frames processed)\n",
      "2025-11-07 14:21:01,709 [INFO] Processing video: person26\n",
      "2025-11-07 14:21:49,446 [INFO] âœ… Finished video: person26 (2460 frames processed)\n",
      "2025-11-07 14:21:49,457 [INFO] Processing video: person27\n",
      "2025-11-07 14:22:36,965 [INFO] âœ… Finished video: person27 (2268 frames processed)\n",
      "2025-11-07 14:22:36,976 [INFO] Processing video: person28\n",
      "2025-11-07 14:23:23,351 [INFO] âœ… Finished video: person28 (2368 frames processed)\n",
      "2025-11-07 14:23:23,361 [INFO] Processing video: person29\n",
      "2025-11-07 14:23:59,449 [INFO] âœ… Finished video: person29 (1869 frames processed)\n",
      "2025-11-07 14:23:59,460 [INFO] Processing video: person3\n",
      "2025-11-07 14:24:46,696 [INFO] âœ… Finished video: person3 (2420 frames processed)\n",
      "2025-11-07 14:24:46,707 [INFO] Processing video: person30\n",
      "2025-11-07 14:25:28,750 [INFO] âœ… Finished video: person30 (2132 frames processed)\n",
      "2025-11-07 14:25:28,760 [INFO] Processing video: person31\n",
      "2025-11-07 14:26:08,638 [INFO] âœ… Finished video: person31 (2022 frames processed)\n",
      "2025-11-07 14:26:08,649 [INFO] Processing video: person32\n",
      "2025-11-07 14:26:52,197 [INFO] âœ… Finished video: person32 (2231 frames processed)\n",
      "2025-11-07 14:26:52,208 [INFO] Processing video: person33\n",
      "2025-11-07 14:27:36,041 [INFO] âœ… Finished video: person33 (2255 frames processed)\n",
      "2025-11-07 14:27:36,052 [INFO] Processing video: person34\n",
      "2025-11-07 14:28:18,665 [INFO] âœ… Finished video: person34 (2177 frames processed)\n",
      "2025-11-07 14:28:18,675 [INFO] Processing video: person35\n",
      "2025-11-07 14:29:05,556 [INFO] âœ… Finished video: person35 (2428 frames processed)\n",
      "2025-11-07 14:29:05,566 [INFO] Processing video: person36\n",
      "2025-11-07 14:29:52,799 [INFO] âœ… Finished video: person36 (2467 frames processed)\n",
      "2025-11-07 14:29:52,810 [INFO] Processing video: person37\n",
      "2025-11-07 14:30:31,067 [INFO] âœ… Finished video: person37 (1985 frames processed)\n",
      "2025-11-07 14:30:31,077 [INFO] Processing video: person38\n",
      "2025-11-07 14:31:09,659 [INFO] âœ… Finished video: person38 (1985 frames processed)\n",
      "2025-11-07 14:31:09,670 [INFO] Processing video: person39\n"
     ]
    }
   ],
   "source": [
    "process_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32d7f212",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd2ad099",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>video</th>\n",
       "      <th>frame_idx</th>\n",
       "      <th>deceptive</th>\n",
       "      <th>Angry</th>\n",
       "      <th>Disgust</th>\n",
       "      <th>Fear</th>\n",
       "      <th>Happy</th>\n",
       "      <th>Sad</th>\n",
       "      <th>Surprise</th>\n",
       "      <th>Neutral</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>person1</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>0.114841</td>\n",
       "      <td>0.114841</td>\n",
       "      <td>0.114841</td>\n",
       "      <td>0.309457</td>\n",
       "      <td>0.114841</td>\n",
       "      <td>0.114841</td>\n",
       "      <td>0.116338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>person1</td>\n",
       "      <td>5</td>\n",
       "      <td>True</td>\n",
       "      <td>0.115004</td>\n",
       "      <td>0.115004</td>\n",
       "      <td>0.115004</td>\n",
       "      <td>0.309557</td>\n",
       "      <td>0.115004</td>\n",
       "      <td>0.115004</td>\n",
       "      <td>0.115422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>person1</td>\n",
       "      <td>10</td>\n",
       "      <td>True</td>\n",
       "      <td>0.114703</td>\n",
       "      <td>0.114703</td>\n",
       "      <td>0.114703</td>\n",
       "      <td>0.311774</td>\n",
       "      <td>0.114703</td>\n",
       "      <td>0.114703</td>\n",
       "      <td>0.114709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>person1</td>\n",
       "      <td>15</td>\n",
       "      <td>True</td>\n",
       "      <td>0.114724</td>\n",
       "      <td>0.114724</td>\n",
       "      <td>0.114724</td>\n",
       "      <td>0.311548</td>\n",
       "      <td>0.114724</td>\n",
       "      <td>0.114724</td>\n",
       "      <td>0.114834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>person1</td>\n",
       "      <td>20</td>\n",
       "      <td>True</td>\n",
       "      <td>0.114609</td>\n",
       "      <td>0.114609</td>\n",
       "      <td>0.114609</td>\n",
       "      <td>0.310822</td>\n",
       "      <td>0.114609</td>\n",
       "      <td>0.114609</td>\n",
       "      <td>0.116133</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     video  frame_idx  deceptive     Angry   Disgust      Fear     Happy  \\\n",
       "0  person1          0       True  0.114841  0.114841  0.114841  0.309457   \n",
       "1  person1          5       True  0.115004  0.115004  0.115004  0.309557   \n",
       "2  person1         10       True  0.114703  0.114703  0.114703  0.311774   \n",
       "3  person1         15       True  0.114724  0.114724  0.114724  0.311548   \n",
       "4  person1         20       True  0.114609  0.114609  0.114609  0.310822   \n",
       "\n",
       "        Sad  Surprise   Neutral  \n",
       "0  0.114841  0.114841  0.116338  \n",
       "1  0.115004  0.115004  0.115422  \n",
       "2  0.114703  0.114703  0.114709  \n",
       "3  0.114724  0.114724  0.114834  \n",
       "4  0.114609  0.114609  0.116133  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('processed_data/silesian_deception_dataset/emotions.csv')\n",
    "df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
